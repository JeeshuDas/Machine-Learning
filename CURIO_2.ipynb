{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c48dd8-b1e8-45ef-9ae7-e78d61dae8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # generate random numbers and random selections\n",
    "import time   # time related function \n",
    "import requests # make web request\n",
    "import hashlib  # get access to hash functions\n",
    "from bs4 import BeautifulSoup # extract info from web page\n",
    "from duckduckgo_search import DDGS # web searching just like google search\n",
    "import chromadb # vector database memory for chatbots helps storing and searching\n",
    "from chromadb.config import Settings\n",
    "import spacy # natural language processing library\n",
    "from sentence_transformers import SentenceTransformer # comparing and storing text\n",
    "from readability import Document # extracting text from web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ded78b0-9e7b-4021-8125-4a04718494d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "nlp = spacy.load(\"en_core_web_sm\") # lode a pre trained nlp model in \"nlp\" variable making it a function to process english text and return an analyzed document\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\") # loads a pre trained transformer model that converts entences to embeddings vectors\n",
    "# here embedding vectors are sentences converted to neumeric data for easier ml model processings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfa53f5-39c3-411b-b733-9a4296352b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB client\n",
    "client = chromadb.Client(Settings()) # creating a vector database for storing the data\n",
    "# here client is the connection to vector database\n",
    "# settings is for customizing storage path, cache, behaviour.\n",
    "collection = client.get_or_create_collection(name=\"web_knowledge\") # web_knowledge is the table/collection name for our database\n",
    "# mainly text embedding takes place here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb1974a-3d2a-4d7b-9a28-5988b468a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory trackers\n",
    "visited = set() # keeps record of visited urls and data acquired\n",
    "hashes = set() # detecting duplicate content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722d93d2-e45e-4faa-993d-a612179bab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting topics\n",
    "topics = [\"Quantum Physics\", \"Ancient Civilizations\", \"Artificial Intelligence\"] # topics to initiate searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8805b5b-3729-420d-9d48-9f8546eff7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDuckGo search function\n",
    "def search_web(topic): # function that takes one topic at a time\n",
    "    print(f\"\\nüîç Searching DuckDuckGo for: {topic}\") # prints the topic whis is being searched\n",
    "    urls = []  # empty lsit to store all the urls for respective topic during the search\n",
    "    try: # try block to catch and handle any error\n",
    "        with DDGS() as ddgs: # DDGS is a class for web searching and ddgs is an instance which actually performs the searching\n",
    "            results = ddgs.text(topic, max_results=5) # results is a dictionary that stors the title, contend and url\n",
    "            # for each topic maximum of 5 url searches would be done\n",
    "            for result in results: # iteration loop using 'result' int dictionary 'results'\n",
    "                href = result.get(\"href\") # to get the href url key from 'results'\n",
    "                if href: # if href exists \n",
    "                    urls.append(href) # add the url in the 'urls' lsit \n",
    "    except Exception as e: # catches any exception while searching\n",
    "        print(f\"[Search Error]: {e}\") # if any exception is found, print search error\n",
    "    return urls # returns the list of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4ecc39-96e2-4860-9f5c-79abddd22cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper using readability\n",
    "def scrape(url): # function to take one url at a time\n",
    "    print(f\"üï∑Ô∏è Scraping: {url}\") # printing the url that is being scrapped\n",
    "    try: # to handle any error during scrapping \n",
    "        r = requests.get(url, timeout=10) # sending a HTTP GET request to url to get data \n",
    "        # 10 seconds for the request to be validated else give up\n",
    "        doc = Document(r.text) # 'r.text' is to extract the redable content from the page\n",
    "        # 'Document' is for removing all unwanted portions and focus on the main content \n",
    "        soup = BeautifulSoup(doc.summary(), 'html.parser') # text extraction happening here\n",
    "        paragraphs = soup.find_all('p') # find all <p> paragraph tags to find out the article \n",
    "        text = ' '.join([p.get_text() for p in paragraphs[:15]]) # extracting text content for first 15 paragraphs\n",
    "        # combine all the extracted paragraphs into one\n",
    "        # limited to 15 paragraphs and is customizable\n",
    "        return text # returns the ultimate content prepared\n",
    "    except Exception as e: # catch any exception\n",
    "        print(f\"[Scrape Error]: {e}\") # if catches, print error\n",
    "        return \"\" # return empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a0829e3-5340-4a3f-82af-70e04aa63870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent duplicate content\n",
    "def is_duplicate(text): # acepts a string 'text'\n",
    "    text_hash = hashlib.sha256(text.encode()).hexdigest() # sha256 is cryptographic hash function that generates 256 bit hash value for any input\n",
    "    # 'text.encode()' converts the text to bytes\n",
    "    # 'a final 256 byte code is generated from the input string'\n",
    "    if text_hash in hashes: # tracks previously seen content \n",
    "        return True # returns true\n",
    "    hashes.add(text_hash) # if new data not seen previously then new hash is added to 'hashes'\n",
    "    return False # hence return false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70069b27-637d-48f0-82c6-7240eac8428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract meaningful keywords\n",
    "def extract_keywords(text): # function accepts text\n",
    "    doc = nlp(text) # using the previously created 'nlp' function/pipeline, keywords are extracted\n",
    "    keywords = list({ent.text for ent in doc.ents if ent.label_ in [\n",
    "        \"PERSON\", \"ORG\", \"GPE\", \"NORP\", \"EVENT\", \"WORK_OF_ART\", \"LANGUAGE\", \"PRODUCT\"\n",
    "    ]}) # extracts the given keywords and stores it in a set (with no duplicate words)\n",
    "    # the set is converted into list\n",
    "    if not keywords: # if the above related keywords not found\n",
    "        keywords = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) < 4] # filters out the long phrases \n",
    "    return keywords[:3] # returns the first 3 keywords (customizable limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588c7f8b-931a-4446-9448-6016fdcc654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store to ChromaDB\n",
    "def store_knowledge(content, topic, url): # function gets the content and its respective topic name and url\n",
    "    embedding = model.encode(content) # converts the text data in neumeric data\n",
    "    collection.add(\n",
    "        documents=[content],\n",
    "        metadatas=[{\"topic\": topic, \"url\": url}],\n",
    "        ids=[str(time.time())],\n",
    "        embeddings=[embedding.tolist()] # stores the data in database \n",
    "    )\n",
    "    print(f\"‚úÖ Stored knowledge from {url}\") # prints the conformation message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f14ce9a4-399f-433d-a8cb-77735a2774f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching DuckDuckGo for: Artificial Intelligence\n",
      "üï∑Ô∏è Scraping: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "‚úÖ Stored knowledge from https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "üß† New Topics Discovered: ['Amazon', 'Alexa', 'AI winters.[9][10] Funding']\n",
      "üï∑Ô∏è Scraping: https://www.britannica.com/technology/artificial-intelligence\n",
      "‚úÖ Stored knowledge from https://www.britannica.com/technology/artificial-intelligence\n",
      "üß† New Topics Discovered: ['\\n\\t\\t\\tFeedback', 'you', 'your feedback']\n",
      "üï∑Ô∏è Scraping: https://www.coursera.org/articles/what-is-artificial-intelligence\n",
      "‚úÖ Stored knowledge from https://www.coursera.org/articles/what-is-artificial-intelligence\n",
      "üß† New Topics Discovered: ['Chat GPT', 'DeepLearning', 'AI']\n",
      "üï∑Ô∏è Scraping: https://www.ibm.com/think/topics/artificial-intelligence\n",
      "‚úÖ Stored knowledge from https://www.ibm.com/think/topics/artificial-intelligence\n",
      "üß† New Topics Discovered: ['Greece', 'German', 'the \"Turing Test']\n",
      "üï∑Ô∏è Scraping: https://www.nasa.gov/what-is-artificial-intelligence/\n",
      "‚úÖ Stored knowledge from https://www.nasa.gov/what-is-artificial-intelligence/\n",
      "üß† New Topics Discovered: ['EO 13960', 'AI', 'Artificial Intelligence']\n",
      "\n",
      "üîç Searching DuckDuckGo for: DeepLearning\n",
      "üï∑Ô∏è Scraping: https://www.deeplearning.ai/\n",
      "‚úÖ Stored knowledge from https://www.deeplearning.ai/\n",
      "üß† New Topics Discovered: ['Andrew Ng', 'AI']\n",
      "üï∑Ô∏è Scraping: https://en.wikipedia.org/wiki/Deep_learning\n",
      "‚úÖ Stored knowledge from https://en.wikipedia.org/wiki/Deep_learning\n",
      "üß† New Topics Discovered: ['CAP', 'Lebesgue', 'Boolean']\n",
      "üï∑Ô∏è Scraping: https://www.geeksforgeeks.org/introduction-deep-learning/\n",
      "‚úÖ Stored knowledge from https://www.geeksforgeeks.org/introduction-deep-learning/\n",
      "üß† New Topics Discovered: ['Multi-Layer Perceptrons', 'Introduction to Deep Learning Deep Learning', 'Deep Learning']\n",
      "üï∑Ô∏è Scraping: https://www.nature.com/articles/nature14539\n",
      "‚úÖ Stored knowledge from https://www.nature.com/articles/nature14539\n",
      "üß† New Topics Discovered: ['C., Couprie, C.,', 'B. & Ramabhadran', 'IEEE Trans']\n",
      "üï∑Ô∏è Scraping: https://d2l.ai/\n",
      "‚úÖ Stored knowledge from https://d2l.ai/\n",
      "üß† New Topics Discovered: ['Nueva Granada', '¬© de Rouen Normandie', 'Teknologi Bandung']\n",
      "\n",
      "üîç Searching DuckDuckGo for: your feedback\n",
      "üï∑Ô∏è Scraping: https://englishrecap.com/ways-to-say-your-feedback-is-important-to-us-in-an-email/\n",
      "‚úÖ Stored knowledge from https://englishrecap.com/ways-to-say-your-feedback-is-important-to-us-in-an-email/\n",
      "üß† New Topics Discovered: ['IP', 'The Website Application Firewall']\n",
      "üï∑Ô∏è Scraping: https://englishrecap.com/polite-ways-to-say-please-provide-feedback/\n",
      "üï∑Ô∏è Scraping: https://englishoverview.com/polite-ways-to-say-please-provide-feedback-with-examples/\n",
      "‚úÖ Stored knowledge from https://englishoverview.com/polite-ways-to-say-please-provide-feedback-with-examples/\n",
      "üß† New Topics Discovered: ['Michael', 'Robert Additional']\n",
      "üï∑Ô∏è Scraping: https://englishrecap.com/ways-to-say-please-let-me-know-your-feedback-in-emails/\n",
      "üï∑Ô∏è Scraping: https://www.broadlearners.com/t/10-alternatives-to-say-we-value-your-feedback/1927\n",
      "‚úÖ Stored knowledge from https://www.broadlearners.com/t/10-alternatives-to-say-we-value-your-feedback/1927\n",
      "üß† New Topics Discovered: ['Customer Support Team', 'Product Development Team']\n",
      "\n",
      "üîç Searching DuckDuckGo for: Quantum Physics\n",
      "üï∑Ô∏è Scraping: https://en.wikipedia.org/wiki/Quantum_mechanics\n",
      "‚úÖ Stored knowledge from https://en.wikipedia.org/wiki/Quantum_mechanics\n",
      "üß† New Topics Discovered: ['Erwin Schr√∂dinger', 'Werner Heisenberg', 'Niels Bohr']\n",
      "üï∑Ô∏è Scraping: https://scienceexchange.caltech.edu/topics/quantum-science-explained/quantum-physics\n",
      "‚úÖ Stored knowledge from https://scienceexchange.caltech.edu/topics/quantum-science-explained/quantum-physics\n",
      "üß† New Topics Discovered: ['Schr√∂dinger']\n",
      "üï∑Ô∏è Scraping: https://www.space.com/quantum-physics-things-you-should-know\n",
      "‚úÖ Stored knowledge from https://www.space.com/quantum-physics-things-you-should-know\n",
      "üß† New Topics Discovered: ['the Nobel Prize', 'Rusty Schweickart', 'Copenhagen']\n",
      "üï∑Ô∏è Scraping: https://www.britannica.com/science/quantum-mechanics-physics\n",
      "‚úÖ Stored knowledge from https://www.britannica.com/science/quantum-mechanics-physics\n",
      "üß† New Topics Discovered: ['quantum mechanics']\n",
      "üï∑Ô∏è Scraping: https://www.thoughtco.com/quantum-physics-overview-2699370\n",
      "[Scrape Error]: HTTPSConnectionPool(host='www.thoughtco.com', port=443): Read timed out. (read timeout=10)\n",
      "\n",
      "üîç Searching DuckDuckGo for: Teknologi Bandung\n",
      "üï∑Ô∏è Scraping: https://en.wikipedia.org/wiki/Bandung_Institute_of_Technology\n",
      "‚úÖ Stored knowledge from https://en.wikipedia.org/wiki/Bandung_Institute_of_Technology\n",
      "üß† New Topics Discovered: ['Indonesian', 'Bandung Institute of Technology', 'German']\n",
      "üï∑Ô∏è Scraping: https://www.itb.ac.id/\n",
      "‚úÖ Stored knowledge from https://www.itb.ac.id/\n",
      "üß† New Topics Discovered: ['Jawa Barat', 'Sumedang', 'Jen']\n",
      "üï∑Ô∏è Scraping: https://www.itb.ac.id/about-itb\n",
      "‚úÖ Stored knowledge from https://www.itb.ac.id/about-itb\n",
      "üß† New Topics Discovered: ['West Java', 'Teknologi Bandung', 'Indonesia']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müß† New Topics Discovered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_topics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     topics\u001b[38;5;241m.\u001b[39mextend(new_topics)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# To avoid rate-limiting\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Continuous learning loop/ an infinite loop\n",
    "while True:\n",
    "    topic = random.choice(topics) # randomly picks up a topic\n",
    "    urls = search_web(topic) # use duck duck go to find url for related topics\n",
    "\n",
    "    for url in urls: # iteration loop\n",
    "        if url in visited: # its checked that the url is visited or not\n",
    "            continue # hence continue or skip that url\n",
    "        visited.add(url) # else add the new url in 'urls' list\n",
    "\n",
    "        content = scrape(url) # here the content from the url is scrapped\n",
    "        if not content or is_duplicate(content): # if duplicate data found \n",
    "            continue # skip\n",
    "\n",
    "        store_knowledge(content, topic, url) # else store the knowledge in database \n",
    "\n",
    "        new_topics = extract_keywords(content) # keywords are extracted from the content and stored in 'new topics'\n",
    "        if new_topics: # if new topic found\n",
    "            print(f\"üß† New Topics Discovered: {new_topics}\") # print new topic found\n",
    "            topics.extend(new_topics) # add the topic and extent the topic list \n",
    "\n",
    "        time.sleep(5)  # before doing next iteration, waits for 5 seconds to avoid overloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff926d-f617-42e9-b58f-5aecf92eb87a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
